<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/consumer_multi_threaded.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/consumer_multi_threaded.py" />
              <option name="originalContent" value="from concurrent.futures import ThreadPoolExecutor, as_completed&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;# Remove the single worker.log - we'll add individual worker logs dynamically&#10;logger.add(&quot;logs/producer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;producer&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/consumer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;consumer&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    print(&quot;\nShutdown signal received. Gracefully shutting down...&quot;)&#10;    logger.info(&quot;Shutdown signal received&quot;)&#10;    shutdown_event.set()&#10;&#10;def consumer_worker(worker_id: int, task_queue: queue.Queue, progress: Progress, rich_task_id: TaskID, stop_event: threading.Event):&#10;    # Add individual log file for this worker&#10;    worker_log_id = logger.add(f&quot;logs/worker_{worker_id}.log&quot;,&#10;                               filter=lambda record: record[&quot;extra&quot;].get(&quot;worker_id&quot;) == worker_id,&#10;                               level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with worker context&#10;    worker_logger = logger.bind(worker_id=worker_id)&#10;    worker_logger.info(f&quot;Worker-{worker_id} started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        worker_logger.debug(f&quot;Processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if stop_event.is_set() or shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(rich_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        worker_logger.debug(f&quot;Completed task {task}&quot;)&#10;&#10;    worker_logger.info(f&quot;Worker-{worker_id} stopping after completing {completed_tasks} tasks&quot;)&#10;&#10;    # Remove the worker's log handler when done&#10;    logger.remove(worker_log_id)&#10;&#10;    return f&quot;Worker-{worker_id} completed {completed_tasks} tasks&quot;&#10;&#10;def producer(task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, producer_task_id: TaskID):&#10;    &quot;&quot;&quot;Producer thread that continuously checks for and adds new tasks&quot;&quot;&quot;&#10;    # Use global logger with producer context&#10;    producer_logger = logger.bind(component=&quot;producer&quot;)&#10;    producer_logger.info(&quot;Producer started&quot;)&#10;&#10;    task_count = 0&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        # Simulate checking for new tasks (e.g., from database, file system, API, etc.)&#10;        if random.random() &lt; 0.7:  # 70% chance of finding a new task&#10;            batch_size = random.randint(3, 6)&#10;            for _ in range(batch_size):&#10;                task_count += 1&#10;                task_queue.put(task_count)&#10;                producer_logger.debug(f&quot;Added task {task_count} to queue&quot;)&#10;                progress.update(producer_task_id, advance=1)&#10;&#10;        time.sleep(random.uniform(0.1, 0.5))  # Check interval&#10;&#10;    producer_logger.info(f&quot;Producer stopping after adding {task_count} tasks&quot;)&#10;    return f&quot;Producer finished after adding {task_count} tasks&quot;&#10;&#10;def consumer(task_queue: queue.Queue, max_workers: int, producer_stop_event: threading.Event, progress: Progress, consumer_task_ids: list):&#10;    &quot;&quot;&quot;Consumer thread that manages worker threads&quot;&quot;&quot;&#10;    # Use global logger with consumer context&#10;    consumer_logger = logger.bind(component=&quot;consumer&quot;)&#10;    consumer_logger.info(f&quot;Consumer started with {max_workers} workers&quot;)&#10;&#10;    worker_stop_event = threading.Event()&#10;&#10;    consumer_logger.debug(&quot;Starting worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=max_workers) as executor:&#10;        futures = [&#10;            executor.submit(consumer_worker, i + 1, task_queue, progress, consumer_task_ids[i], worker_stop_event)&#10;            for i in range(max_workers)&#10;        ]&#10;&#10;        # Wait for producer to finish or shutdown signal&#10;        while not producer_stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        if shutdown_event.is_set():&#10;            consumer_logger.debug(&quot;Shutdown event set, stopping workers&quot;)&#10;        elif producer_stop_event.is_set():&#10;            consumer_logger.debug(&quot;Producer stopped, signaling workers to stop&quot;)&#10;        elif task_queue.empty():&#10;            consumer_logger.debug(&quot;Task queue is empty, signaling workers to stop&quot;)&#10;        else:&#10;            consumer_logger.debug(&quot;Signaling workers to stop&quot;)&#10;&#10;        # Signal workers to stop&#10;        worker_stop_event.set()&#10;&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            consumer_logger.debug(f&quot;Worker finished: {result}&quot;)&#10;&#10;    consumer_logger.info(&quot;Consumer finished&quot;)&#10;    return f&quot;Consumer finished managing {max_workers} workers&quot;&#10;&#10;def main():&#10;    # Add console handler for main function only (no filter means it gets all messages, but we'll only use it for main)&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: &quot;worker_id&quot; not in record[&quot;extra&quot;] and record[&quot;extra&quot;].get(&quot;component&quot;) is None)&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    max_workers = 5&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and stop event for producer&#10;    task_queue = queue.Queue()&#10;    for i in range(50):&#10;        task_queue.put(i)&#10;    logger.info(f&quot;Initialized queue with 50 tasks&quot;)&#10;&#10;    producer_stop_event = threading.Event()&#10;&#10;    try:&#10;        logger.info(&quot;Starting producer and consumer threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;            # Create progress task for producer&#10;            producer_task_id = progress.add_task(&#10;                description=&quot;[green]Producer&quot;,&#10;                total=None&#10;            )&#10;&#10;            # Create progress tasks for consumer workers&#10;            consumer_task_ids = []&#10;            for i in range(max_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[blue]Consumer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                consumer_task_ids.append(task_id)&#10;&#10;            # Start producer and consumer threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=2) as main_executor:&#10;                # Start producer thread&#10;                producer_future = main_executor.submit(producer, task_queue, producer_stop_event, progress, producer_task_id)&#10;&#10;                # Start consumer thread&#10;                consumer_future = main_executor.submit(consumer, task_queue, max_workers, producer_stop_event, progress, consumer_task_ids)&#10;&#10;                # Wait for shutdown signal&#10;                while not shutdown_event.is_set():&#10;                    time.sleep(0.1)&#10;&#10;                # Stop producer&#10;                producer_stop_event.set()&#10;                logger.info(&quot;Stopping producer...&quot;)&#10;&#10;                # Wait for both to complete&#10;                producer_result = producer_future.result()&#10;                consumer_result = consumer_future.result()&#10;&#10;        # Log results after Progress context exits to avoid interference&#10;        logger.info(f&quot;Producer result: {producer_result}&quot;)&#10;        logger.info(f&quot;Consumer result: {consumer_result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;    finally:&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="from concurrent.futures import ThreadPoolExecutor, as_completed&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;# Remove the single worker.log - we'll add individual worker logs dynamically&#10;logger.add(&quot;logs/producer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;producer&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/consumer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;consumer&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;# Global progress instance for signal handler&#10;progress_instance = None&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    if progress_instance:&#10;        progress_instance.console.print(&quot;\n[yellow]Shutdown signal received. Gracefully shutting down...[/yellow]&quot;)&#10;    logger.info(&quot;Shutdown signal received&quot;)&#10;    shutdown_event.set()&#10;&#10;def consumer_worker(worker_id: int, task_queue: queue.Queue, progress: Progress, rich_task_id: TaskID, stop_event: threading.Event):&#10;    # Add individual log file for this worker&#10;    worker_log_id = logger.add(f&quot;logs/worker_{worker_id}.log&quot;,&#10;                               filter=lambda record: record[&quot;extra&quot;].get(&quot;worker_id&quot;) == worker_id,&#10;                               level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with worker context&#10;    worker_logger = logger.bind(worker_id=worker_id)&#10;    worker_logger.info(f&quot;Worker-{worker_id} started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        worker_logger.debug(f&quot;Processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if stop_event.is_set() or shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(rich_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        worker_logger.debug(f&quot;Completed task {task}&quot;)&#10;&#10;    worker_logger.info(f&quot;Worker-{worker_id} stopping after completing {completed_tasks} tasks&quot;)&#10;&#10;    # Remove the worker's log handler when done&#10;    logger.remove(worker_log_id)&#10;&#10;    return f&quot;Worker-{worker_id} completed {completed_tasks} tasks&quot;&#10;&#10;def producer(task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, producer_task_id: TaskID):&#10;    &quot;&quot;&quot;Producer thread that continuously checks for and adds new tasks&quot;&quot;&quot;&#10;    # Use global logger with producer context&#10;    producer_logger = logger.bind(component=&quot;producer&quot;)&#10;    producer_logger.info(&quot;Producer started&quot;)&#10;&#10;    task_count = 0&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        # Simulate checking for new tasks (e.g., from database, file system, API, etc.)&#10;        if random.random() &lt; 0.7:  # 70% chance of finding a new task&#10;            batch_size = random.randint(3, 6)&#10;            for _ in range(batch_size):&#10;                task_count += 1&#10;                task_queue.put(task_count)&#10;                producer_logger.debug(f&quot;Added task {task_count} to queue&quot;)&#10;                progress.update(producer_task_id, advance=1)&#10;&#10;        time.sleep(random.uniform(0.1, 0.5))  # Check interval&#10;&#10;    producer_logger.info(f&quot;Producer stopping after adding {task_count} tasks&quot;)&#10;    return f&quot;Producer finished after adding {task_count} tasks&quot;&#10;&#10;def consumer(task_queue: queue.Queue, max_workers: int, producer_stop_event: threading.Event, progress: Progress, consumer_task_ids: list):&#10;    &quot;&quot;&quot;Consumer thread that manages worker threads&quot;&quot;&quot;&#10;    # Use global logger with consumer context&#10;    consumer_logger = logger.bind(component=&quot;consumer&quot;)&#10;    consumer_logger.info(f&quot;Consumer started with {max_workers} workers&quot;)&#10;&#10;    worker_stop_event = threading.Event()&#10;&#10;    consumer_logger.debug(&quot;Starting worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=max_workers) as executor:&#10;        futures = [&#10;            executor.submit(consumer_worker, i + 1, task_queue, progress, consumer_task_ids[i], worker_stop_event)&#10;            for i in range(max_workers)&#10;        ]&#10;&#10;        # Wait for producer to finish or shutdown signal&#10;        while not producer_stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        if shutdown_event.is_set():&#10;            consumer_logger.debug(&quot;Shutdown event set, stopping workers&quot;)&#10;        elif producer_stop_event.is_set():&#10;            consumer_logger.debug(&quot;Producer stopped, signaling workers to stop&quot;)&#10;        elif task_queue.empty():&#10;            consumer_logger.debug(&quot;Task queue is empty, signaling workers to stop&quot;)&#10;        else:&#10;            consumer_logger.debug(&quot;Signaling workers to stop&quot;)&#10;&#10;        # Signal workers to stop&#10;        worker_stop_event.set()&#10;&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            consumer_logger.debug(f&quot;Worker finished: {result}&quot;)&#10;&#10;    consumer_logger.info(&quot;Consumer finished&quot;)&#10;    return f&quot;Consumer finished managing {max_workers} workers&quot;&#10;&#10;def main():&#10;    global progress_instance&#10;    &#10;    # Add console handler for main function only (no filter means it gets all messages, but we'll only use it for main)&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: &quot;worker_id&quot; not in record[&quot;extra&quot;] and record[&quot;extra&quot;].get(&quot;component&quot;) is None)&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    max_workers = 5&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and stop event for producer&#10;    task_queue = queue.Queue()&#10;    for i in range(50):&#10;        task_queue.put(i)&#10;    logger.info(f&quot;Initialized queue with 50 tasks&quot;)&#10;&#10;    producer_stop_event = threading.Event()&#10;&#10;    try:&#10;        logger.info(&quot;Starting producer and consumer threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;            progress_instance = progress  # Make it accessible to signal handler&#10;            &#10;            # Create progress task for producer&#10;            producer_task_id = progress.add_task(&#10;                description=&quot;[green]Producer&quot;,&#10;                total=None&#10;            )&#10;&#10;            # Create progress tasks for consumer workers&#10;            consumer_task_ids = []&#10;            for i in range(max_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[blue]Consumer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                consumer_task_ids.append(task_id)&#10;&#10;            # Start producer and consumer threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=2) as main_executor:&#10;                # Start producer thread&#10;                producer_future = main_executor.submit(producer, task_queue, producer_stop_event, progress, producer_task_id)&#10;&#10;                # Start consumer thread&#10;                consumer_future = main_executor.submit(consumer, task_queue, max_workers, producer_stop_event, progress, consumer_task_ids)&#10;&#10;                # Wait for shutdown signal&#10;                while not shutdown_event.is_set():&#10;                    time.sleep(0.1)&#10;&#10;                # Stop producer&#10;                producer_stop_event.set()&#10;                logger.info(&quot;Stopping producer...&quot;)&#10;&#10;                # Wait for both to complete&#10;                producer_result = producer_future.result()&#10;                consumer_result = consumer_future.result()&#10;&#10;        # Clear the global reference&#10;        progress_instance = None&#10;&#10;        # Log results after Progress context exits to avoid interference&#10;        logger.info(f&quot;Producer result: {producer_result}&quot;)&#10;        logger.info(f&quot;Consumer result: {consumer_result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;    finally:&#10;        progress_instance = None&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="originalContent" value="import typer&#10;from typing_extensions import Annotated&#10;from enum import Enum&#10;&#10;class VariantChoice(str, Enum):&#10;    single = &quot;single&quot;&#10;    consumer_multi = &quot;consumer-multi&quot;&#10;    producer_multi = &quot;producer-multi&quot;&#10;    both_multi = &quot;both-multi&quot;&#10;&#10;app = typer.Typer(help=&quot;Task processing system with different threading variants&quot;)&#10;&#10;@app.command()&#10;def run(&#10;    variant: Annotated[VariantChoice, typer.Argument(help=&quot;Choose which variant to run&quot;)] = VariantChoice.single&#10;):&#10;    &quot;&quot;&quot;&#10;    Run the task processing system with the specified variant.&#10;&#10;    Variants:&#10;    - single: Single-threaded producer and consumer&#10;    - consumer-multi: Multi-threaded consumer, single-threaded producer&#10;    - producer-multi: Multi-threaded producer, single-threaded consumer&#10;    - both-multi: Multi-threaded producer and consumer&#10;    &quot;&quot;&quot;&#10;&#10;    typer.echo(f&quot;Starting {variant.value} variant...&quot;)&#10;&#10;    if variant == VariantChoice.single:&#10;        from single_threaded import main&#10;        main()&#10;    elif variant == VariantChoice.consumer_multi:&#10;        from consumer_multi_threaded import main&#10;        main()&#10;    elif variant == VariantChoice.producer_multi:&#10;        from producer_multi_threaded import main&#10;        main()&#10;    elif variant == VariantChoice.both_multi:&#10;        from multi_threaded import main&#10;        main()&#10;&#10;@app.command()&#10;def list_variants():&#10;    &quot;&quot;&quot;List all available variants with descriptions.&quot;&quot;&quot;&#10;    typer.echo(&quot;Available variants:&quot;)&#10;    typer.echo(&quot;  single           - Single-threaded producer and consumer&quot;)&#10;    typer.echo(&quot;  consumer-multi   - Multi-threaded consumer, single-threaded producer&quot;)&#10;    typer.echo(&quot;  producer-multi   - Multi-threaded producer, single-threaded consumer&quot;)&#10;    typer.echo(&quot;  both-multi       - Multi-threaded producer and consumer&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    app()&#10;&#10;" />
              <option name="updatedContent" value="import typer&#10;from typing_extensions import Annotated&#10;from enum import Enum&#10;&#10;class VariantChoice(str, Enum):&#10;    single = &quot;single&quot;&#10;    consumer_multi = &quot;consumer-multi&quot;&#10;    producer_multi = &quot;producer-multi&quot;&#10;    both_multi = &quot;both-multi&quot;&#10;    only_workers = &quot;only-workers&quot;&#10;&#10;app = typer.Typer(help=&quot;Task processing system with different threading variants&quot;)&#10;&#10;@app.command()&#10;def run(&#10;    variant: Annotated[VariantChoice, typer.Argument(help=&quot;Choose which variant to run&quot;)] = VariantChoice.single&#10;):&#10;    &quot;&quot;&quot;&#10;    Run the task processing system with the specified variant.&#10;&#10;    Variants:&#10;    - single: Single-threaded producer and consumer&#10;    - consumer-multi: Multi-threaded consumer, single-threaded producer&#10;    - producer-multi: Multi-threaded producer, single-threaded consumer&#10;    - both-multi: Multi-threaded producer and consumer&#10;    - only-workers: Workers process tasks from a pre-populated queue&#10;    &quot;&quot;&quot;&#10;&#10;    typer.echo(f&quot;Starting {variant.value} variant...&quot;)&#10;&#10;    if variant == VariantChoice.single:&#10;        from single_threaded import main&#10;        main()&#10;    elif variant == VariantChoice.consumer_multi:&#10;        from consumer_multi_threaded import main&#10;        main()&#10;    elif variant == VariantChoice.producer_multi:&#10;        from producer_multi_threaded import main&#10;        main()&#10;    elif variant == VariantChoice.both_multi:&#10;        from multi_threaded import main&#10;        main()&#10;    elif variant == VariantChoice.only_workers:&#10;        from only_workers import main&#10;        main()&#10;&#10;@app.command()&#10;def list_variants():&#10;    &quot;&quot;&quot;List all available variants with descriptions.&quot;&quot;&quot;&#10;    typer.echo(&quot;Available variants:&quot;)&#10;    typer.echo(&quot;  single           - Single-threaded producer and consumer&quot;)&#10;    typer.echo(&quot;  consumer-multi   - Multi-threaded consumer, single-threaded producer&quot;)&#10;    typer.echo(&quot;  producer-multi   - Multi-threaded producer, single-threaded consumer&quot;)&#10;    typer.echo(&quot;  both-multi       - Multi-threaded producer and consumer&quot;)&#10;    typer.echo(&quot;  only-workers     - Workers process tasks from a pre-populated queue&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    app()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/multi_threaded.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/multi_threaded.py" />
              <option name="originalContent" value="from concurrent.futures import ThreadPoolExecutor, as_completed&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;# Remove the single worker.log - we'll add individual worker logs dynamically&#10;logger.add(&quot;logs/producer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;producer&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/consumer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;consumer&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    print(&quot;\nShutdown signal received. Gracefully shutting down...&quot;)&#10;    logger.info(&quot;Shutdown signal received&quot;)&#10;    shutdown_event.set()&#10;&#10;def consumer_worker(worker_id: int, task_queue: queue.Queue, progress: Progress, rich_task_id: TaskID, stop_event: threading.Event):&#10;    # Add individual log file for this worker&#10;    worker_log_id = logger.add(f&quot;logs/consumer_worker_{worker_id}.log&quot;,&#10;                               filter=lambda record: record[&quot;extra&quot;].get(&quot;worker_id&quot;) == worker_id,&#10;                               level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with worker context&#10;    worker_logger = logger.bind(worker_id=worker_id)&#10;    worker_logger.info(f&quot;Consumer-Worker-{worker_id} started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        worker_logger.debug(f&quot;Processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if stop_event.is_set() or shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(rich_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        worker_logger.debug(f&quot;Completed task {task}&quot;)&#10;&#10;    worker_logger.info(f&quot;Consumer-Worker-{worker_id} stopping after completing {completed_tasks} tasks&quot;)&#10;&#10;    # Remove the worker's log handler when done&#10;    logger.remove(worker_log_id)&#10;&#10;    return f&quot;Consumer-Worker-{worker_id} completed {completed_tasks} tasks&quot;&#10;&#10;def producer_worker(worker_id: int, task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, rich_task_id: TaskID):&#10;    &quot;&quot;&quot;Producer worker thread that adds tasks to the queue&quot;&quot;&quot;&#10;    # Add individual log file for this producer worker&#10;    producer_worker_log_id = logger.add(f&quot;logs/producer_worker_{worker_id}.log&quot;,&#10;                                        filter=lambda record: record[&quot;extra&quot;].get(&quot;producer_worker_id&quot;) == worker_id,&#10;                                        level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with producer worker context&#10;    producer_worker_logger = logger.bind(producer_worker_id=worker_id)&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} started&quot;)&#10;&#10;    task_count = 0&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        # Simulate checking for new tasks (e.g., from database, file system, API, etc.)&#10;        if random.random() &lt; 0.8:  # 80% chance of finding new tasks&#10;            batch_size = random.randint(2, 5)&#10;            for _ in range(batch_size):&#10;                task_count += 1&#10;                # Use worker_id to create unique task IDs across producer workers&#10;                unique_task_id = f&quot;{worker_id}-{task_count}&quot;&#10;                task_queue.put(unique_task_id)&#10;                producer_worker_logger.debug(f&quot;Added task {unique_task_id} to queue&quot;)&#10;                progress.update(rich_task_id, advance=1)&#10;&#10;        time.sleep(random.uniform(0.05, 0.2))  # Check interval&#10;&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} stopping after adding {task_count} tasks&quot;)&#10;&#10;    # Remove the producer worker's log handler when done&#10;    logger.remove(producer_worker_log_id)&#10;&#10;    return f&quot;Producer-Worker-{worker_id} added {task_count} tasks&quot;&#10;&#10;def producer(task_queue: queue.Queue, max_workers: int, stop_event: threading.Event, progress: Progress, producer_task_ids: list):&#10;    &quot;&quot;&quot;Producer thread that manages multiple producer workers&quot;&quot;&quot;&#10;    # Use global logger with producer context&#10;    producer_logger = logger.bind(component=&quot;producer&quot;)&#10;    producer_logger.info(f&quot;Producer started with {max_workers} workers&quot;)&#10;&#10;    producer_worker_stop_event = threading.Event()&#10;&#10;    producer_logger.debug(&quot;Starting producer worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=max_workers) as executor:&#10;        futures = [&#10;            executor.submit(producer_worker, i+1, task_queue, producer_worker_stop_event, progress, producer_task_ids[i])&#10;            for i in range(max_workers)&#10;        ]&#10;&#10;        # Wait for stop signal&#10;        while not stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        producer_logger.info(&quot;Signaling producer workers to stop&quot;)&#10;        producer_worker_stop_event.set()&#10;&#10;        total_tasks_added = 0&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            producer_logger.debug(f&quot;Producer worker finished: {result}&quot;)&#10;            # Extract task count from result string&#10;            task_count = int(result.split()[-2])&#10;            total_tasks_added += task_count&#10;&#10;    producer_logger.info(f&quot;Producer finished - total {total_tasks_added} tasks added by all workers&quot;)&#10;    return f&quot;Producer finished after adding {total_tasks_added} tasks&quot;&#10;&#10;def consumer(task_queue: queue.Queue, max_workers: int, producer_stop_event: threading.Event, progress: Progress, consumer_task_ids: list):&#10;    &quot;&quot;&quot;Consumer thread that manages worker threads and progress display&quot;&quot;&quot;&#10;    # Use global logger with consumer context&#10;    consumer_logger = logger.bind(component=&quot;consumer&quot;)&#10;    consumer_logger.info(f&quot;Consumer started with {max_workers} workers&quot;)&#10;&#10;    worker_stop_event = threading.Event()&#10;&#10;    consumer_logger.debug(&quot;Starting worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=max_workers) as executor:&#10;        futures = [&#10;            executor.submit(consumer_worker, i + 1, task_queue, progress, consumer_task_ids[i], worker_stop_event)&#10;            for i in range(max_workers)&#10;        ]&#10;&#10;        # Wait for producer to finish or shutdown signal&#10;        while not producer_stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        if shutdown_event.is_set():&#10;            consumer_logger.debug(&quot;Shutdown event set, stopping workers&quot;)&#10;        elif producer_stop_event.is_set():&#10;            consumer_logger.debug(&quot;Producer stopped, signaling workers to stop&quot;)&#10;        elif task_queue.empty():&#10;            consumer_logger.debug(&quot;Task queue is empty, signaling workers to stop&quot;)&#10;        else:&#10;            consumer_logger.debug(&quot;Signaling workers to stop&quot;)&#10;&#10;        # Signal workers to stop&#10;        worker_stop_event.set()&#10;&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            consumer_logger.debug(f&quot;Worker finished: {result}&quot;)&#10;&#10;    consumer_logger.info(&quot;Consumer finished&quot;)&#10;    return f&quot;Consumer finished managing {max_workers} workers&quot;&#10;&#10;def main():&#10;    # Add console handler for main function only&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: &quot;worker_id&quot; not in record[&quot;extra&quot;] and record[&quot;extra&quot;].get(&quot;component&quot;) is None and &quot;producer_worker_id&quot; not in record[&quot;extra&quot;])&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    num_consumer_workers = 5&#10;    num_producer_workers = 3&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and stop event for producer&#10;    task_queue = queue.Queue()&#10;    for i in range(20):  # Reduced initial tasks since producer will add more&#10;        task_queue.put(f&quot;initial-{i}&quot;)&#10;    logger.info(f&quot;Initialized queue with 20 initial tasks&quot;)&#10;&#10;    producer_stop_event = threading.Event()&#10;&#10;    try:&#10;        logger.info(f&quot;Starting producer ({num_producer_workers} workers) and consumer ({num_consumer_workers} workers) threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;            # Create progress tasks for producer workers&#10;            producer_task_ids = []&#10;            for i in range(num_producer_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[green]Producer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                producer_task_ids.append(task_id)&#10;&#10;            # Create progress tasks for consumer workers&#10;            consumer_task_ids = []&#10;            for i in range(num_consumer_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[blue]Consumer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                consumer_task_ids.append(task_id)&#10;&#10;            # Start producer and consumer threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=2) as main_executor:&#10;                # Start producer thread with multiple workers&#10;                producer_future = main_executor.submit(producer, task_queue, num_producer_workers, producer_stop_event, progress, producer_task_ids)&#10;&#10;                # Start consumer thread&#10;                consumer_future = main_executor.submit(consumer, task_queue, num_consumer_workers, producer_stop_event, progress, consumer_task_ids)&#10;&#10;                # Wait for shutdown signal&#10;                while not shutdown_event.is_set():&#10;                    time.sleep(0.1)&#10;&#10;                # Stop producer&#10;                producer_stop_event.set()&#10;                logger.info(&quot;Stopping producer...&quot;)&#10;&#10;                # Wait for both to complete&#10;                producer_result = producer_future.result()&#10;                consumer_result = consumer_future.result()&#10;&#10;        # Log results after Progress context exits to avoid interference&#10;        logger.info(f&quot;Producer result: {producer_result}&quot;)&#10;        logger.info(f&quot;Consumer result: {consumer_result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;    finally:&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="from concurrent.futures import ThreadPoolExecutor, as_completed&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;# Remove the single worker.log - we'll add individual worker logs dynamically&#10;logger.add(&quot;logs/producer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;producer&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/consumer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;consumer&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;# Global progress instance for signal handler&#10;progress_instance = None&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    if progress_instance:&#10;        progress_instance.console.print(&quot;\n[yellow]Shutdown signal received. Gracefully shutting down...[/yellow]&quot;)&#10;    logger.info(&quot;Shutdown signal received&quot;)&#10;    shutdown_event.set()&#10;&#10;def consumer_worker(worker_id: int, task_queue: queue.Queue, progress: Progress, rich_task_id: TaskID, stop_event: threading.Event):&#10;    # Add individual log file for this worker&#10;    worker_log_id = logger.add(f&quot;logs/consumer_worker_{worker_id}.log&quot;,&#10;                               filter=lambda record: record[&quot;extra&quot;].get(&quot;worker_id&quot;) == worker_id,&#10;                               level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with worker context&#10;    worker_logger = logger.bind(worker_id=worker_id)&#10;    worker_logger.info(f&quot;Consumer-Worker-{worker_id} started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        worker_logger.debug(f&quot;Processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if stop_event.is_set() or shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(rich_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        worker_logger.debug(f&quot;Completed task {task}&quot;)&#10;&#10;    worker_logger.info(f&quot;Consumer-Worker-{worker_id} stopping after completing {completed_tasks} tasks&quot;)&#10;&#10;    # Remove the worker's log handler when done&#10;    logger.remove(worker_log_id)&#10;&#10;    return f&quot;Consumer-Worker-{worker_id} completed {completed_tasks} tasks&quot;&#10;&#10;def producer_worker(worker_id: int, task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, rich_task_id: TaskID):&#10;    &quot;&quot;&quot;Producer worker thread that adds tasks to the queue&quot;&quot;&quot;&#10;    # Add individual log file for this producer worker&#10;    producer_worker_log_id = logger.add(f&quot;logs/producer_worker_{worker_id}.log&quot;,&#10;                                        filter=lambda record: record[&quot;extra&quot;].get(&quot;producer_worker_id&quot;) == worker_id,&#10;                                        level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with producer worker context&#10;    producer_worker_logger = logger.bind(producer_worker_id=worker_id)&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} started&quot;)&#10;&#10;    task_count = 0&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        # Simulate checking for new tasks (e.g., from database, file system, API, etc.)&#10;        if random.random() &lt; 0.8:  # 80% chance of finding new tasks&#10;            batch_size = random.randint(2, 5)&#10;            for _ in range(batch_size):&#10;                task_count += 1&#10;                # Use worker_id to create unique task IDs across producer workers&#10;                unique_task_id = f&quot;{worker_id}-{task_count}&quot;&#10;                task_queue.put(unique_task_id)&#10;                producer_worker_logger.debug(f&quot;Added task {unique_task_id} to queue&quot;)&#10;                progress.update(rich_task_id, advance=1)&#10;&#10;        time.sleep(random.uniform(0.05, 0.2))  # Check interval&#10;&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} stopping after adding {task_count} tasks&quot;)&#10;&#10;    # Remove the producer worker's log handler when done&#10;    logger.remove(producer_worker_log_id)&#10;&#10;    return f&quot;Producer-Worker-{worker_id} added {task_count} tasks&quot;&#10;&#10;def producer(task_queue: queue.Queue, max_workers: int, stop_event: threading.Event, progress: Progress, producer_task_ids: list):&#10;    &quot;&quot;&quot;Producer thread that manages multiple producer workers&quot;&quot;&quot;&#10;    # Use global logger with producer context&#10;    producer_logger = logger.bind(component=&quot;producer&quot;)&#10;    producer_logger.info(f&quot;Producer started with {max_workers} workers&quot;)&#10;&#10;    producer_worker_stop_event = threading.Event()&#10;&#10;    producer_logger.debug(&quot;Starting producer worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=max_workers) as executor:&#10;        futures = [&#10;            executor.submit(producer_worker, i+1, task_queue, producer_worker_stop_event, progress, producer_task_ids[i])&#10;            for i in range(max_workers)&#10;        ]&#10;&#10;        # Wait for stop signal&#10;        while not stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        producer_logger.info(&quot;Signaling producer workers to stop&quot;)&#10;        producer_worker_stop_event.set()&#10;&#10;        total_tasks_added = 0&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            producer_logger.debug(f&quot;Producer worker finished: {result}&quot;)&#10;            # Extract task count from result string&#10;            task_count = int(result.split()[-2])&#10;            total_tasks_added += task_count&#10;&#10;    producer_logger.info(f&quot;Producer finished - total {total_tasks_added} tasks added by all workers&quot;)&#10;    return f&quot;Producer finished after adding {total_tasks_added} tasks&quot;&#10;&#10;def consumer(task_queue: queue.Queue, max_workers: int, producer_stop_event: threading.Event, progress: Progress, consumer_task_ids: list):&#10;    &quot;&quot;&quot;Consumer thread that manages worker threads and progress display&quot;&quot;&quot;&#10;    # Use global logger with consumer context&#10;    consumer_logger = logger.bind(component=&quot;consumer&quot;)&#10;    consumer_logger.info(f&quot;Consumer started with {max_workers} workers&quot;)&#10;&#10;    worker_stop_event = threading.Event()&#10;&#10;    consumer_logger.debug(&quot;Starting worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=max_workers) as executor:&#10;        futures = [&#10;            executor.submit(consumer_worker, i + 1, task_queue, progress, consumer_task_ids[i], worker_stop_event)&#10;            for i in range(max_workers)&#10;        ]&#10;&#10;        # Wait for producer to finish or shutdown signal&#10;        while not producer_stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        if shutdown_event.is_set():&#10;            consumer_logger.debug(&quot;Shutdown event set, stopping workers&quot;)&#10;        elif producer_stop_event.is_set():&#10;            consumer_logger.debug(&quot;Producer stopped, signaling workers to stop&quot;)&#10;        elif task_queue.empty():&#10;            consumer_logger.debug(&quot;Task queue is empty, signaling workers to stop&quot;)&#10;        else:&#10;            consumer_logger.debug(&quot;Signaling workers to stop&quot;)&#10;&#10;        # Signal workers to stop&#10;        worker_stop_event.set()&#10;&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            consumer_logger.debug(f&quot;Worker finished: {result}&quot;)&#10;&#10;    consumer_logger.info(&quot;Consumer finished&quot;)&#10;    return f&quot;Consumer finished managing {max_workers} workers&quot;&#10;&#10;def main():&#10;    global progress_instance&#10;    &#10;    # Add console handler for main function only&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: &quot;worker_id&quot; not in record[&quot;extra&quot;] and record[&quot;extra&quot;].get(&quot;component&quot;) is None and &quot;producer_worker_id&quot; not in record[&quot;extra&quot;])&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    num_consumer_workers = 5&#10;    num_producer_workers = 3&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and stop event for producer&#10;    task_queue = queue.Queue()&#10;    for i in range(20):  # Reduced initial tasks since producer will add more&#10;        task_queue.put(f&quot;initial-{i}&quot;)&#10;    logger.info(f&quot;Initialized queue with 20 initial tasks&quot;)&#10;&#10;    producer_stop_event = threading.Event()&#10;&#10;    try:&#10;        logger.info(f&quot;Starting producer ({num_producer_workers} workers) and consumer ({num_consumer_workers} workers) threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;            progress_instance = progress  # Make it accessible to signal handler&#10;            &#10;            # Create progress tasks for producer workers&#10;            producer_task_ids = []&#10;            for i in range(num_producer_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[green]Producer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                producer_task_ids.append(task_id)&#10;&#10;            # Create progress tasks for consumer workers&#10;            consumer_task_ids = []&#10;            for i in range(num_consumer_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[blue]Consumer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                consumer_task_ids.append(task_id)&#10;&#10;            # Start producer and consumer threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=2) as main_executor:&#10;                # Start producer thread with multiple workers&#10;                producer_future = main_executor.submit(producer, task_queue, num_producer_workers, producer_stop_event, progress, producer_task_ids)&#10;&#10;                # Start consumer thread&#10;                consumer_future = main_executor.submit(consumer, task_queue, num_consumer_workers, producer_stop_event, progress, consumer_task_ids)&#10;&#10;                # Wait for shutdown signal&#10;                while not shutdown_event.is_set():&#10;                    time.sleep(0.1)&#10;&#10;                # Stop producer&#10;                producer_stop_event.set()&#10;                logger.info(&quot;Stopping producer...&quot;)&#10;&#10;                # Wait for both to complete&#10;                producer_result = producer_future.result()&#10;                consumer_result = consumer_future.result()&#10;&#10;        # Clear the global reference&#10;        progress_instance = None&#10;&#10;        # Log results after Progress context exits to avoid interference&#10;        logger.info(f&quot;Producer result: {producer_result}&quot;)&#10;        logger.info(f&quot;Consumer result: {consumer_result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;    finally:&#10;        progress_instance = None&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/only_workers.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/only_workers.py" />
              <option name="originalContent" value="from concurrent.futures import ThreadPoolExecutor, TimeoutError&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/worker.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;worker&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    logger.info(&quot;Shutdown signal received. Gracefully shutting down...&quot;)&#10;    shutdown_event.set()&#10;&#10;def worker(worker_id: int, task_queue: queue.Queue, progress: Progress, worker_task_id: TaskID):&#10;    &quot;&quot;&quot;Worker thread that processes tasks from the queue&quot;&quot;&quot;&#10;    worker_logger = logger.bind(component=&quot;worker&quot;)&#10;    worker_logger.info(f&quot;Worker {worker_id} started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not shutdown_event.is_set():&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        worker_logger.debug(f&quot;Worker {worker_id} processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(worker_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        worker_logger.debug(f&quot;Worker {worker_id} completed task {task}&quot;)&#10;&#10;    worker_logger.info(f&quot;Worker {worker_id} stopping after completing {completed_tasks} tasks&quot;)&#10;    return f&quot;Worker {worker_id} completed {completed_tasks} tasks&quot;&#10;&#10;def main():&#10;    # Add console handler for main function only&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) is None)&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and populate with all tasks upfront&#10;    task_queue = queue.Queue()&#10;    total_tasks = 50  # Create 50 tasks to process&#10;    for i in range(total_tasks):&#10;        task_queue.put(f&quot;task-{i+1}&quot;)&#10;    logger.info(f&quot;Created queue with {total_tasks} tasks&quot;)&#10;&#10;    num_workers = 4  # Number of worker threads&#10;&#10;    try:&#10;        logger.info(f&quot;Starting {num_workers} worker threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;&#10;            # Create progress tasks for each worker&#10;            worker_task_ids = []&#10;            for i in range(num_workers):&#10;                worker_task_id = progress.add_task(&#10;                    description=f&quot;[cyan]Worker {i+1}&quot;,&#10;                    total=None&#10;                )&#10;                worker_task_ids.append(worker_task_id)&#10;&#10;            # Start worker threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=num_workers) as executor:&#10;                # Start all worker threads&#10;                worker_futures = []&#10;                for i in range(num_workers):&#10;                    future = executor.submit(worker, i+1, task_queue, progress, worker_task_ids[i])&#10;                    worker_futures.append(future)&#10;&#10;                # Wait for all workers to complete or shutdown signal&#10;                results = []&#10;                for future in worker_futures:&#10;                    try:&#10;                        # Use a short timeout to check for shutdown periodically&#10;                        result = future.result(timeout=0.5)&#10;                        results.append(result)&#10;                    except TimeoutError:&#10;                        # If shutdown was signaled, still try to get the result&#10;                        if shutdown_event.is_set():&#10;                            try:&#10;                                result = future.result(timeout=1.0)  # Give it a bit more time&#10;                                results.append(result)&#10;                            except TimeoutError:&#10;                                results.append(f&quot;Worker timed out during shutdown&quot;)&#10;                        else:&#10;                            # Continue waiting if no shutdown signal&#10;                            result = future.result()&#10;                            results.append(result)&#10;&#10;        # Log results after Progress context exits to avoid interference&#10;        for result in results:&#10;            logger.info(f&quot;Worker result: {result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;        # Still try to collect any available results&#10;        if 'worker_futures' in locals():&#10;            for future in worker_futures:&#10;                try:&#10;                    result = future.result(timeout=0.1)&#10;                    logger.info(f&quot;Worker result: {result}&quot;)&#10;                except:&#10;                    pass&#10;    finally:&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="from concurrent.futures import ThreadPoolExecutor, TimeoutError&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/worker.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;worker&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;# Global progress instance for signal handler&#10;progress_instance = None&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    if progress_instance:&#10;        progress_instance.console.print(&quot;\n[yellow]Shutdown signal received. Gracefully shutting down...[/yellow]&quot;)&#10;    logger.info(&quot;Shutdown signal received. Gracefully shutting down...&quot;)&#10;    shutdown_event.set()&#10;&#10;def worker(worker_id: int, task_queue: queue.Queue, progress: Progress, worker_task_id: TaskID):&#10;    &quot;&quot;&quot;Worker thread that processes tasks from the queue&quot;&quot;&quot;&#10;    worker_logger = logger.bind(component=&quot;worker&quot;)&#10;    worker_logger.info(f&quot;Worker {worker_id} started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not shutdown_event.is_set():&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        worker_logger.debug(f&quot;Worker {worker_id} processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(worker_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        worker_logger.debug(f&quot;Worker {worker_id} completed task {task}&quot;)&#10;&#10;    worker_logger.info(f&quot;Worker {worker_id} stopping after completing {completed_tasks} tasks&quot;)&#10;    return f&quot;Worker {worker_id} completed {completed_tasks} tasks&quot;&#10;&#10;def main():&#10;    global progress_instance&#10;    &#10;    # Add console handler for main function only&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) is None)&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and populate with all tasks upfront&#10;    task_queue = queue.Queue()&#10;    total_tasks = 50  # Create 50 tasks to process&#10;    for i in range(total_tasks):&#10;        task_queue.put(f&quot;task-{i+1}&quot;)&#10;    logger.info(f&quot;Created queue with {total_tasks} tasks&quot;)&#10;&#10;    num_workers = 4  # Number of worker threads&#10;&#10;    try:&#10;        logger.info(f&quot;Starting {num_workers} worker threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;            progress_instance = progress  # Make it accessible to signal handler&#10;&#10;            # Create progress tasks for each worker&#10;            worker_task_ids = []&#10;            for i in range(num_workers):&#10;                worker_task_id = progress.add_task(&#10;                    description=f&quot;[cyan]Worker {i+1}&quot;,&#10;                    total=None&#10;                )&#10;                worker_task_ids.append(worker_task_id)&#10;&#10;            # Start worker threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=num_workers) as executor:&#10;                # Start all worker threads&#10;                worker_futures = []&#10;                for i in range(num_workers):&#10;                    future = executor.submit(worker, i+1, task_queue, progress, worker_task_ids[i])&#10;                    worker_futures.append(future)&#10;&#10;                # Wait for all workers to complete or shutdown signal&#10;                results = []&#10;                for future in worker_futures:&#10;                    try:&#10;                        # Use a short timeout to check for shutdown periodically&#10;                        result = future.result(timeout=0.5)&#10;                        results.append(result)&#10;                    except TimeoutError:&#10;                        # If shutdown was signaled, still try to get the result&#10;                        if shutdown_event.is_set():&#10;                            try:&#10;                                result = future.result(timeout=1.0)  # Give it a bit more time&#10;                                results.append(result)&#10;                            except TimeoutError:&#10;                                results.append(f&quot;Worker timed out during shutdown&quot;)&#10;                        else:&#10;                            # Continue waiting if no shutdown signal&#10;                            result = future.result()&#10;                            results.append(result)&#10;&#10;        # Clear the global reference&#10;        progress_instance = None&#10;        &#10;        # Log results after Progress context exits to avoid interference&#10;        for result in results:&#10;            logger.info(f&quot;Worker result: {result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;        # Still try to collect any available results&#10;        if 'worker_futures' in locals():&#10;            for future in worker_futures:&#10;                try:&#10;                    result = future.result(timeout=0.1)&#10;                    logger.info(f&quot;Worker result: {result}&quot;)&#10;                except:&#10;                    pass&#10;    finally:&#10;        progress_instance = None&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/producer_multi_threaded.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/producer_multi_threaded.py" />
              <option name="originalContent" value="from concurrent.futures import ThreadPoolExecutor, as_completed&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;# Remove the single worker.log - we'll add individual worker logs dynamically&#10;logger.add(&quot;logs/producer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;producer&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/consumer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;consumer&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    print(&quot;\nShutdown signal received. Gracefully shutting down...&quot;)&#10;    logger.info(&quot;Shutdown signal received&quot;)&#10;    shutdown_event.set()&#10;&#10;def producer_worker(worker_id: int, task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, rich_task_id: TaskID):&#10;    &quot;&quot;&quot;Producer worker thread that adds tasks to the queue&quot;&quot;&quot;&#10;    # Add individual log file for this producer worker&#10;    producer_worker_log_id = logger.add(f&quot;logs/producer_worker_{worker_id}.log&quot;,&#10;                                        filter=lambda record: record[&quot;extra&quot;].get(&quot;producer_worker_id&quot;) == worker_id,&#10;                                        level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with producer worker context&#10;    producer_worker_logger = logger.bind(producer_worker_id=worker_id)&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} started&quot;)&#10;&#10;    task_count = 0&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        # Simulate checking for new tasks (e.g., from database, file system, API, etc.)&#10;        if random.random() &lt; 0.8:  # 80% chance of finding new tasks&#10;            batch_size = random.randint(2, 5)&#10;            for _ in range(batch_size):&#10;                task_count += 1&#10;                # Use worker_id to create unique task IDs across producer workers&#10;                unique_task_id = f&quot;{worker_id}-{task_count}&quot;&#10;                task_queue.put(unique_task_id)&#10;                producer_worker_logger.debug(f&quot;Added task {unique_task_id} to queue&quot;)&#10;                progress.update(rich_task_id, advance=1)&#10;&#10;        time.sleep(random.uniform(0.05, 0.2))  # Check interval&#10;&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} stopping after adding {task_count} tasks&quot;)&#10;&#10;    # Remove the producer worker's log handler when done&#10;    logger.remove(producer_worker_log_id)&#10;&#10;    return f&quot;Producer-Worker-{worker_id} added {task_count} tasks&quot;&#10;&#10;def producer(task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, producer_task_ids: list, num_producer_workers: int = 3):&#10;    &quot;&quot;&quot;Producer thread that manages multiple producer workers&quot;&quot;&quot;&#10;    # Use global logger with producer context&#10;    producer_logger = logger.bind(component=&quot;producer&quot;)&#10;    producer_logger.info(f&quot;Producer started with {num_producer_workers} workers&quot;)&#10;&#10;    producer_worker_stop_event = threading.Event()&#10;&#10;    producer_logger.debug(&quot;Starting producer worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=num_producer_workers) as executor:&#10;        futures = [&#10;            executor.submit(producer_worker, i+1, task_queue, producer_worker_stop_event, progress, producer_task_ids[i])&#10;            for i in range(num_producer_workers)&#10;        ]&#10;&#10;        # Wait for stop signal&#10;        while not stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        producer_logger.info(&quot;Signaling producer workers to stop&quot;)&#10;        producer_worker_stop_event.set()&#10;&#10;        total_tasks_added = 0&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            producer_logger.debug(f&quot;Producer worker finished: {result}&quot;)&#10;            # Extract task count from result string&#10;            task_count = int(result.split()[-2])&#10;            total_tasks_added += task_count&#10;&#10;    producer_logger.info(f&quot;Producer finished - total {total_tasks_added} tasks added by all workers&quot;)&#10;    return f&quot;Producer finished after adding {total_tasks_added} tasks&quot;&#10;&#10;def consumer(task_queue: queue.Queue, producer_stop_event: threading.Event, progress: Progress, consumer_task_id: TaskID):&#10;    &quot;&quot;&quot;Consumer thread that processes tasks from the queue&quot;&quot;&quot;&#10;    # Use global logger with consumer context&#10;    consumer_logger = logger.bind(component=&quot;consumer&quot;)&#10;    consumer_logger.info(&quot;Consumer started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not producer_stop_event.is_set() or not task_queue.empty():&#10;        if shutdown_event.is_set():&#10;            break&#10;&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        consumer_logger.debug(f&quot;Processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(consumer_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        consumer_logger.debug(f&quot;Completed task {task}&quot;)&#10;&#10;    consumer_logger.info(f&quot;Consumer stopping after completing {completed_tasks} tasks&quot;)&#10;    return f&quot;Consumer completed {completed_tasks} tasks&quot;&#10;&#10;def main():&#10;    # Add console handler for main function only&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: &quot;worker_id&quot; not in record[&quot;extra&quot;] and record[&quot;extra&quot;].get(&quot;component&quot;) is None and &quot;producer_worker_id&quot; not in record[&quot;extra&quot;])&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    num_producer_workers = 3&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and stop event for producer&#10;    task_queue = queue.Queue()&#10;    for i in range(20):  # Reduced initial tasks since producer will add more&#10;        task_queue.put(f&quot;initial-{i}&quot;)&#10;    logger.info(f&quot;Initialized queue with 20 initial tasks&quot;)&#10;&#10;    producer_stop_event = threading.Event()&#10;&#10;    try:&#10;        logger.info(f&quot;Starting producer ({num_producer_workers} workers) and consumer threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;            # Create progress tasks for producer workers&#10;            producer_task_ids = []&#10;            for i in range(num_producer_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[green]Producer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                producer_task_ids.append(task_id)&#10;&#10;            # Create single progress task for consumer&#10;            consumer_task_id = progress.add_task(&#10;                description=&quot;[blue]Consumer&quot;,&#10;                total=None&#10;            )&#10;&#10;            # Start producer and consumer threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=2) as main_executor:&#10;                # Start producer thread with multiple workers&#10;                producer_future = main_executor.submit(producer, task_queue, producer_stop_event, progress, producer_task_ids, num_producer_workers)&#10;&#10;                # Start consumer thread&#10;                consumer_future = main_executor.submit(consumer, task_queue, producer_stop_event, progress, consumer_task_id)&#10;&#10;                # Wait for shutdown signal&#10;                while not shutdown_event.is_set():&#10;                    time.sleep(0.1)&#10;&#10;                # Stop producer&#10;                producer_stop_event.set()&#10;                logger.info(&quot;Stopping producer...&quot;)&#10;&#10;                # Wait for both to complete&#10;                producer_result = producer_future.result()&#10;                consumer_result = consumer_future.result()&#10;&#10;        # Log results after Progress context exits to avoid interference&#10;        logger.info(f&quot;Producer result: {producer_result}&quot;)&#10;        logger.info(f&quot;Consumer result: {consumer_result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;    finally:&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="from concurrent.futures import ThreadPoolExecutor, as_completed&#10;from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TaskID&#10;import time&#10;import random&#10;import queue&#10;import threading&#10;import signal&#10;import sys&#10;from loguru import logger&#10;import os&#10;&#10;# Configure loguru with filters&#10;logger.remove()  # Remove default handler&#10;logger.add(&quot;logs/app.log&quot;, rotation=&quot;10 MB&quot;, retention=&quot;7 days&quot;, level=&quot;DEBUG&quot;)&#10;# Remove the single worker.log - we'll add individual worker logs dynamically&#10;logger.add(&quot;logs/producer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;producer&quot;, level=&quot;DEBUG&quot;)&#10;logger.add(&quot;logs/consumer.log&quot;, filter=lambda record: record[&quot;extra&quot;].get(&quot;component&quot;) == &quot;consumer&quot;, level=&quot;DEBUG&quot;)&#10;&#10;# Global event for graceful shutdown&#10;shutdown_event = threading.Event()&#10;# Global progress instance for signal handler&#10;progress_instance = None&#10;&#10;def signal_handler(signum, frame):&#10;    &quot;&quot;&quot;Handle Ctrl+C signal for graceful shutdown&quot;&quot;&quot;&#10;    if progress_instance:&#10;        progress_instance.console.print(&quot;\n[yellow]Shutdown signal received. Gracefully shutting down...[/yellow]&quot;)&#10;    logger.info(&quot;Shutdown signal received&quot;)&#10;    shutdown_event.set()&#10;&#10;def producer_worker(worker_id: int, task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, rich_task_id: TaskID):&#10;    &quot;&quot;&quot;Producer worker thread that adds tasks to the queue&quot;&quot;&quot;&#10;    # Add individual log file for this producer worker&#10;    producer_worker_log_id = logger.add(f&quot;logs/producer_worker_{worker_id}.log&quot;,&#10;                                        filter=lambda record: record[&quot;extra&quot;].get(&quot;producer_worker_id&quot;) == worker_id,&#10;                                        level=&quot;DEBUG&quot;)&#10;&#10;    # Use global logger with producer worker context&#10;    producer_worker_logger = logger.bind(producer_worker_id=worker_id)&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} started&quot;)&#10;&#10;    task_count = 0&#10;    while not stop_event.is_set() and not shutdown_event.is_set():&#10;        # Simulate checking for new tasks (e.g., from database, file system, API, etc.)&#10;        if random.random() &lt; 0.8:  # 80% chance of finding new tasks&#10;            batch_size = random.randint(2, 5)&#10;            for _ in range(batch_size):&#10;                task_count += 1&#10;                # Use worker_id to create unique task IDs across producer workers&#10;                unique_task_id = f&quot;{worker_id}-{task_count}&quot;&#10;                task_queue.put(unique_task_id)&#10;                producer_worker_logger.debug(f&quot;Added task {unique_task_id} to queue&quot;)&#10;                progress.update(rich_task_id, advance=1)&#10;&#10;        time.sleep(random.uniform(0.05, 0.2))  # Check interval&#10;&#10;    producer_worker_logger.info(f&quot;Producer-Worker-{worker_id} stopping after adding {task_count} tasks&quot;)&#10;&#10;    # Remove the producer worker's log handler when done&#10;    logger.remove(producer_worker_log_id)&#10;&#10;    return f&quot;Producer-Worker-{worker_id} added {task_count} tasks&quot;&#10;&#10;def producer(task_queue: queue.Queue, stop_event: threading.Event, progress: Progress, producer_task_ids: list, num_producer_workers: int = 3):&#10;    &quot;&quot;&quot;Producer thread that manages multiple producer workers&quot;&quot;&quot;&#10;    # Use global logger with producer context&#10;    producer_logger = logger.bind(component=&quot;producer&quot;)&#10;    producer_logger.info(f&quot;Producer started with {num_producer_workers} workers&quot;)&#10;&#10;    producer_worker_stop_event = threading.Event()&#10;&#10;    producer_logger.debug(&quot;Starting producer worker threads&quot;)&#10;    with ThreadPoolExecutor(max_workers=num_producer_workers) as executor:&#10;        futures = [&#10;            executor.submit(producer_worker, i+1, task_queue, producer_worker_stop_event, progress, producer_task_ids[i])&#10;            for i in range(num_producer_workers)&#10;        ]&#10;&#10;        # Wait for stop signal&#10;        while not stop_event.is_set() and not shutdown_event.is_set():&#10;            time.sleep(0.1)&#10;&#10;        producer_logger.info(&quot;Signaling producer workers to stop&quot;)&#10;        producer_worker_stop_event.set()&#10;&#10;        total_tasks_added = 0&#10;        for future in as_completed(futures):&#10;            result = future.result()&#10;            producer_logger.debug(f&quot;Producer worker finished: {result}&quot;)&#10;            # Extract task count from result string&#10;            task_count = int(result.split()[-2])&#10;            total_tasks_added += task_count&#10;&#10;    producer_logger.info(f&quot;Producer finished - total {total_tasks_added} tasks added by all workers&quot;)&#10;    return f&quot;Producer finished after adding {total_tasks_added} tasks&quot;&#10;&#10;def consumer(task_queue: queue.Queue, producer_stop_event: threading.Event, progress: Progress, consumer_task_id: TaskID):&#10;    &quot;&quot;&quot;Consumer thread that processes tasks from the queue&quot;&quot;&quot;&#10;    # Use global logger with consumer context&#10;    consumer_logger = logger.bind(component=&quot;consumer&quot;)&#10;    consumer_logger.info(&quot;Consumer started&quot;)&#10;&#10;    completed_tasks = 0&#10;&#10;    while not producer_stop_event.is_set() or not task_queue.empty():&#10;        if shutdown_event.is_set():&#10;            break&#10;&#10;        try:&#10;            task = task_queue.get(timeout=0.1)&#10;        except queue.Empty:&#10;            break&#10;&#10;        consumer_logger.debug(f&quot;Processing task {task}&quot;)&#10;        # Simulate work for this task&#10;        work_steps = random.randint(5, 15)&#10;        for _ in range(work_steps):&#10;            if shutdown_event.is_set():&#10;                break&#10;            time.sleep(random.uniform(0.01, 0.05))&#10;&#10;        progress.update(consumer_task_id, advance=1)&#10;        completed_tasks += 1&#10;        task_queue.task_done()&#10;        consumer_logger.debug(f&quot;Completed task {task}&quot;)&#10;&#10;    consumer_logger.info(f&quot;Consumer stopping after completing {completed_tasks} tasks&quot;)&#10;    return f&quot;Consumer completed {completed_tasks} tasks&quot;&#10;&#10;def main():&#10;    global progress_instance&#10;    &#10;    # Add console handler for main function only&#10;    main_logger_id = logger.add(sys.stderr, level=&quot;INFO&quot;,&#10;                                format=&quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;main&lt;/cyan&gt; | {message}&quot;,&#10;                                filter=lambda record: &quot;worker_id&quot; not in record[&quot;extra&quot;] and record[&quot;extra&quot;].get(&quot;component&quot;) is None and &quot;producer_worker_id&quot; not in record[&quot;extra&quot;])&#10;&#10;    logger.info(&quot;Application starting&quot;)&#10;&#10;    # Set up signal handler for graceful shutdown&#10;    signal.signal(signal.SIGINT, signal_handler)&#10;&#10;    num_producer_workers = 3&#10;&#10;    # Create logs directory if it doesn't exist&#10;    os.makedirs(&quot;logs&quot;, exist_ok=True)&#10;&#10;    # Create task queue and stop event for producer&#10;    task_queue = queue.Queue()&#10;    for i in range(20):  # Reduced initial tasks since producer will add more&#10;        task_queue.put(f&quot;initial-{i}&quot;)&#10;    logger.info(f&quot;Initialized queue with 20 initial tasks&quot;)&#10;&#10;    producer_stop_event = threading.Event()&#10;&#10;    try:&#10;        logger.info(f&quot;Starting producer ({num_producer_workers} workers) and consumer threads&quot;)&#10;&#10;        # Create a single shared Progress instance&#10;        with Progress(&#10;            TextColumn(&quot;[bold]{task.description}&quot;, justify=&quot;right&quot;),&#10;            BarColumn(),&#10;            TextColumn(&quot;{task.completed} tasks&quot;),&#10;            TimeElapsedColumn(),&#10;        ) as progress:&#10;            progress_instance = progress  # Make it accessible to signal handler&#10;            &#10;            # Create progress tasks for producer workers&#10;            producer_task_ids = []&#10;            for i in range(num_producer_workers):&#10;                task_id = progress.add_task(&#10;                    description=f&quot;[green]Producer-{i+1}&quot;,&#10;                    total=None&#10;                )&#10;                producer_task_ids.append(task_id)&#10;&#10;            # Create single progress task for consumer&#10;            consumer_task_id = progress.add_task(&#10;                description=&quot;[blue]Consumer&quot;,&#10;                total=None&#10;            )&#10;&#10;            # Start producer and consumer threads using ThreadPoolExecutor&#10;            with ThreadPoolExecutor(max_workers=2) as main_executor:&#10;                # Start producer thread with multiple workers&#10;                producer_future = main_executor.submit(producer, task_queue, producer_stop_event, progress, producer_task_ids, num_producer_workers)&#10;&#10;                # Start consumer thread&#10;                consumer_future = main_executor.submit(consumer, task_queue, producer_stop_event, progress, consumer_task_id)&#10;&#10;                # Wait for shutdown signal&#10;                while not shutdown_event.is_set():&#10;                    time.sleep(0.1)&#10;&#10;                # Stop producer&#10;                producer_stop_event.set()&#10;                logger.info(&quot;Stopping producer...&quot;)&#10;&#10;                # Wait for both to complete&#10;                producer_result = producer_future.result()&#10;                consumer_result = consumer_future.result()&#10;&#10;        # Clear the global reference&#10;        progress_instance = None&#10;&#10;        # Log results after Progress context exits to avoid interference&#10;        logger.info(f&quot;Producer result: {producer_result}&quot;)&#10;        logger.info(f&quot;Consumer result: {consumer_result}&quot;)&#10;        logger.info(&quot;All tasks completed!&quot;)&#10;&#10;    except KeyboardInterrupt:&#10;        logger.warning(&quot;Forced shutdown...&quot;)&#10;        shutdown_event.set()&#10;    finally:&#10;        progress_instance = None&#10;        logger.info(&quot;Application shutdown complete&quot;)&#10;        logger.remove(main_logger_id)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>